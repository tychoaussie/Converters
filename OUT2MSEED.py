# -*- coding: utf-8 -*-

'''The MIT License (MIT)

Copyright (c) 2013 Daniel Burk
during my time on campus during a summer of delightful albeit, uncompensated labor.
Michigan State University.

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.'''


__author__ = "Daniel Burk <burkdani@msu.edu>"
__version__ = "20170517"
__license__ = "MIT"

# -*- coding: utf-8 -*-
# 20170517 - Removed the need for SAC file writing from the code and now just go straight to miniseed.
#
# 20170516 - Stop trying to calculate sample rate and just trust the one procided in the vdaq
#             file. THe sample rate GPS pulse is not always consistent and sometimes has glitches
#             causing the cample rate calculator to fail. There's no easy fix, so just go with
#             the published sample rate, which seems to be more or less correct.

# 20170510 - Corrected a timing bug in the calculation of sample rate and seconds remainder.
# OUT files incorrectly report start time when the timing pulse occurs within the first four
# or five samples. This requires knowledge of the previous file's calculateed end time.
# As a result, this program must have a time continuous list of files. 
 

# 20170203 - update to use the new SAC file engine from LANL and OBSPY verson 1.01
# Now outputs to Miniseed with STEIM2 compression, and creates a more sensible, trackable
# file name
# 20150817 - updated to run on latest Obspy version plus bring in channel names into
# the SAC filename. Also correct the start time bug by using the encoded start time
# from the first two samples of the timing signal.

import sys, os, csv, time, string, subprocess, numpy as np
from obspy.core import read, Trace, Stream, UTCDateTime
# from obspy.io.sac import SACTrace


# Now, the most important part -- The legalese:
# COPYRIGHT ©  BOARD OF TRUSTEES OF MICHIGAN STATE UNIVERSITY
# ALL RIGHTS RESERVED

# PERMISSION IS GRANTED TO USE, COPY, COMBINE AND/OR MERGE, CREATE DERIVATIVE
# WORKS AND REDISTRIBUTE THIS SOFTWARE AND SUCH DERIVATIVE WORKS FOR ANY PURPOSE,
# SO LONG AS THE NAME OF MICHIGAN STATE UNIVERSITY IS NOT USED IN ANY ADVERTISING
# OR PUBLICITY PERTAINING TO THE USE OR DISTRIBUTION OF THIS SOFTWARE WITHOUT 
# SPECIFIC, WRITTEN PRIOR AUTHORIZATION.  IF THE ABOVE COPYRIGHT NOTICE OR ANY
# OTHER IDENTIFICATION OF MICHIGAN STATE UNIVERSITY IS INCLUDED IN ANY COPY OF 
# ANY PORTION OF THIS SOFTWARE, THEN THE DISCLAIMER BELOW MUST ALSO BE INCLUDED.

# THIS SOFTWARE IS PROVIDED AS IS, WITHOUT REPRESENTATION FROM MICHIGAN STATE
# UNIVERSITY AS TO ITS FITNESS FOR ANY PURPOSE, AND WITHOUT WARRANTY BY MICHIGAN
# STATE UNIVERSITY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT
# LIMITATION THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
# PARTICULAR PURPOSE.

# THE MICHIGAN STATE UNIVERSITY BOARD OF TRUSTEES SHALL NOT BE LIABLE FOR ANY
# DAMAGES, INCLUDING SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL DAMAGES,
# WITH RESPECT TO ANY CLAIM ARISING OUT OF OR IN CONNECTION WITH THE USE OF
# THE SOFTWARE, EVEN IF IT HAS BEEN OR IS HEREAFTER ADVISED OF THE POSSIBILITY
# OF SUCH DAMAGES.


#class OUTconvertError(Exception):
#
#   def __init__(self, error_name, error_text):
#       self.args = (error_name, error_text)
#       self.error_name = error_name
#       self.error_text = error_text
#
#   def __str__(self):
#       return repr('{0}: {1}'.format(self.error_name, self.error_text)


class OUTconvert(object):
    '''OUT2SAC is a utility for converting OUT binary files as generated by
       the Russian seismic networks that use Symmetric Research seismic hardware.
       The OUT files are converted to SAC file format for integration within other
       seismic analysis tools. At present, only OUT files generated in version 1.14 
       and later are compatable with the converter. Future iterations of this code
       should also include early OUT files as well as conventional Symres DAT files.

       Syntax:     OUT2SAC infile (outfile)
       or,
       Syntax:     OUT2SAC Targetdirectory (destinationdirectory)
       
       where:      infile may be either a file name or a file directory.
       Outfile is optional. If no outfile is specified, SAC files shall be
       named the same as the original infile but with a "_x" on the end of file
       name where x represents the sequence of each channel within the series.
       It is a requirement that the infile be co-located in a directory
       which also contains the file "vdaq.txt" that describes the data
       file header information.

       Useage requirements: You must have installed both Python27, ObsPy, 
       and NumPy on your machine in order to run this package. Dependencies
       include the os, csv, time, string, numpy, and obspy modules.

       Typical useage:
       c:\Python27\scripts> python OUT2sac.py c:/data/20130816112559.OUT
       c:\Python27\scripts> python OUT2sac.py c:/data c:/data/outputfiles

       '''
#
#    OUTconvert is a utility for converting ascii files generated by Symmetric Research 
#       and Russian network .OUT files into a ObsPy compatable format. 
#       Once reformatted, ObsPy may be used to convert the data into SAC or even
#       Miniseed format.
#       
#       Useage: Activate Python's ObsPy package, then from the command prompt
#       specify the target file and the output filename. If no output filename
#       is provided, the default will be called output.ASC in your current
#       working directory. If a directory is specified, all OUT files found within
#       will be converted. If no command line options are specified, the format will
#       default to SAC-BIN. The target directory in all cases must contain at least
#       one OUT file and one vdaq.txt file with appropriate header information.#
#
#       Syntax: OUTconvert infile_name outfile_name (-sac) (-seed)#
#
#       -sac is an optional command line switch for also creating a series of SAC files
#       from your ascii dataset as specified by the input.
#       -seed is an optional command line switch for also creating a series of SEED files
#       from your ascii dataset as specified by the input.
#
#       Typical useage:
#       <ObsPy> C:\Python27\scripts> python OUTconvert.py /../infil.ASC /../out.ASC -sac
#
#      OUTconvert:
#      Soubrouting that opens the binary file infile, uses the header information generated 
#      by the vdaq subroutine, and creates an ObsPy compatable ASC file.
# 

def mkgmtime(t):         # This function makes an inverse for the time tuple to convert back to a floating point epoch
    """Convert UTC tuple to seconds since Epoch"""
    return time.mktime(t)-time.timezone


def vdaq(headerfile):
    #
    #vdaq seeks the headerfile and loads the appropriate variables as defined by the Magadan
    # network vdaq.txt, a modification of symmetric research's dat file format.
    
    with open(headerfile,'r') as head:
        headdata = csv.reader(head)
        header = {}
        for row in headdata:
            r = row[0].split()
            # Load a dictionary of header file information
            header[r[0]] = r[1]   
    return header




def convert(Prev_E_time,Prev_E_time_Frac_second,infile,outfolder,Network):
            #
            #       Look for the header file by parsing out the directory. 
            #       If it doesn't exist, the program will error.
            #       Pass in the previous file's end time to compare to the start time of this file
            #       Bring in the infile, and the specified out file, and the network name.
            #

#    print "The infile specified into the conversion is {}".format(infile)
#    print "The outfile specified into the conversion is {}".format(outfile)
    folder = infile[:infile.rfind('\\')+1]
#    outfolder = infile[:infile.rfind('\\')+1]
    headerfile = folder+'vdaq.txt'
    
    hexfile = infile   #  
    header = vdaq(headerfile)
    Samplecount = header['RecordPts:']


        # Extract the start time to the nearest second from the file name
        # File name is an established standard of 14 characters
        # hexfile[-18:-4] represents st.tiome to nearest second
        # 20130314000509
        # Note!! This is computer system time, NOT the start time. So don't do this.
        # Use the start time as encoded within the timing channel (channel 0) as found
        # within the first two samples.

        #    St_time = time.strptime(hexfile[-18:-4],"%Y%m%d%H%M%S")
 
            # Import the binary data
            # Each channel sample comprises of four bytes
            # Epoch time is taken from bytes 1, 0, 13, 12 in that order.
            # Create a data type object of four channels each of which consist of a 32bit integer

    dt = np.dtype([(header['Ch0ID:'],np.int32),(header['Ch1ID:'],np.int32),(header['Ch2ID:'],np.int32),(header['Ch3ID:'],np.int32)])

            # Calculate sample rate and seconds remainder for adding onto file start time.
            # Load timing signal into an array and calculate mean. 
            # Find first sample representing the first positive edge trigger that's greater than sample 5
            # Note that if signal starts high, it must drop low before counting.
            # Count the number of excursions where timing signal goes initially high, starting with the second timing signal
            # and en
            # Find the first sample where gps tick-mark goes high.
            # If tickmark is already high on the 4th sample, go to the next tick mark and count back.


    Data = np.fromfile(hexfile,dtype = dt)      # load all data from the binary file using our specified format dt
    try:
            # Data[0][0] represents MSBaLSBa 0000 of epoch start time from gps
            # Data[1][0] represents MSBbLSBb 0000 of epoch start time from gps
            # Epoch start time must be arranged thus: MSBa LSBa MSBb LSBb            
            # Note that rest of the Data is simply a list of raw counts from the ADC and is a 32 bit integer value.
            # It would be nice if each channel was converted to a measurement in terms of volts as a float value.
            # The Symres PAR4CH system is 24 bits for +-10V, and the USB4CH is listed as +-4V
            # but practical measurements in the lab put it more like +-8V. 
            # Therefore, this code is going to ASSUME a nominal value of 0.94 microvolts / count.
            # This converter will convert raw counts into millivolts using this gain factor. Future versions
            # will enable us to input the gain factor as it becomes available.
            #
            #
            # Channelgain = [0.94*1e-6,0.94*1e-6,0.94*1e-6,0.94*1e-6] # volts per count
            #

        GPS = []                   # declare our GPS stream which will be loaded from the Data
        Latch = False
        Count = -1                                  # First time Count is incremented is on tic mark number zero.
        Initial_sample = 0
        Final_sample = 0
        Frac_second = 0.0
        Sps = 0.0
        units = ['Volts   ','Volts   ','Volts   ','Volts   ']
        comment = ['TIME    ','Velocity','Velocity','Velocity']
        data = []
        data.append(Data[0][0]) # MSB of start time from file
        data.append(Data[1][0]) # LSB of start time from file


        for n in range(len(Data)):                  # Load the GPS array
            GPS.append(Data[n][0])
        Gpsmean15 = (2.0 * np.median(GPS))            # Use a value that is 2.0 times the median as the pulse break


            # Check to see if the signal started out on a high pulse
        if GPS[4] > np.mean(GPS):
            Latch = True                # Set latch as rising edge has been missed

        for n in range (5,(len(GPS))):
            if (Latch == True):             # Look for falling edge to reset latch#
                if GPS[n] < Gpsmean15:
                    Latch = False
            else:             
                if GPS[n] > Gpsmean15:
                    Latch = True        # Rising edge found so set latch
                    Count += 1          # and increment edge count starting at zero.
                    if Initial_sample == 0:
                        Initial_sample = n  # Set the first known good rising edge
                    else:    
                        Final_sample = n    # Keep updating last known good rising edge


#        Sps = float(int(float(Final_sample-Initial_sample)/Count)*1000)/1000
        Sps = float(header['SampleRate:'])
        print 'Sample rate set to {}'.format(Sps) 
# Use the vdaq sample rate as above method turned out to be unreliable when
# the GPS signal was glitchy.

        Delta = float(int((1/Sps)*1000))/1000   # Round the Delta to the nearest millisecond.


            #                       Calculate time remainder which equals 
            #                 1000 milliseconds - (#samples before first impulse)

        if (Initial_sample - Sps) == 4 : # Meaning, there are more samples to the edge than
                                    # can be accounted for in one second

            Frac_second = 1 - ((Initial_sample - Sps-1)/Sps)
            print "Initial Sample - SPS = {0}. Fraction of second calculated to {1} sec.".format((Initial_sample - Sps),Frac_second)
 
        elif  (Initial_sample - Sps) == 3 or (Initial_sample - Sps) == 2: # Meaning, there are more samples to the edge than
                                                                    # can be accounted for in one second

            Frac_second = 1 - ((Initial_sample - Sps-1)/Sps)
            print "Initial Sample - SPS = {0}. Fraction of second calculated to {1} sec.".format((Initial_sample - Sps),Frac_second)
            print "Backing up the timing by one second to compensate for OUT file timing glitch."
            data[1] = data[1]-1 # Decrement the timer by one second, as the original digitizer
                            # missed the mark.

        elif (Initial_sample - Sps) == 1: # meaning the onset of the edge occurs on the first sample
            Frac_second = 0.0
#        data[1] = data[1]-2 # Decrement the timer by one second as original digitizer missed the mark
#            print "Initial Sample - SPS = {0}. Fraction of second = {1} sec.".format((Initial_sample - Sps),Frac_second)

#    elif (Initial_sample - Sps) == -33: # For unknown reasons, when 33 samples in, the timer jumps to the wrong second.
#        Frac_second = 1 - ((Initial_sample-1) / Sps)
#        data[1] = data[1]-1 # Decrement the timer by one second, as the original digitizer
                            # was tripped up by the falling edge of previous timing mark

        else: 
            Frac_second = 1 - ((Initial_sample-1) / Sps)
#            print "Initial Sample - SPS = {0}. Frac = {1} sec.".format((Initial_sample - Sps),Frac_second)


        timestamp = long(int(data[0])<<16|int(data[1])) # Assemble them into the timestamp
        St_time = time.gmtime(timestamp) # Convert them into a tuple representing start time to nearest second   

#
#                    Validate the timing using the end time of the previously converted file and fix, if necessary
#                    Prev_E_time, Prev_E_Frac_second
#                    Convert the start time and the previous file time into a float value and take the difference.
#                    They should vary by only one sample width.
#                    If the difference is negative, than the start time is too far back in time by one second. Add a second
#                    If the difference is positive, than the start time is too far forward in time by one second. Subtract a second
#                    It should be close to zero, and if it is, leave well enough alone.

        starttime = mkgmtime(St_time)+Frac_second # This is the proposed start time of the file in epoch with the fraction

#        print "The unmodified Prev_E_time = {0}.{1:0.3f} which is {2}.{3:0.3f}".format( \
#                mkgmtime(Prev_E_time), \
#                Prev_E_time_Frac_second, \
#                time.strftime("%Y_%m_%d_%H_%M_%S_",Prev_E_time), \
#                Prev_E_time_Frac_second)

        prev_endtime = mkgmtime(Prev_E_time) + Prev_E_time_Frac_second

#    print  "New end time = {0}{1}".format(time.strftime("%Y_%m_%d_%H_%M_%S_",time.gmtime(prev_endtime)),Prev_E_time_Frac_second)

        if prev_endtime > 0.999: # and ((Initial_sample - Sps) == 1):  # If the start time is less than one, it is invalid and this is the first file of the sequence.   
            time_difference = starttime - prev_endtime

            print "Proposed start time = {0}{1:0.3f}.\nPrevious end time = {2}{3:0.3f}. Difference = {4:1.3f}".format( \
                 time.strftime("%Y_%m_%d_%H_%M_%S_",St_time),\
                 Frac_second, \
                 time.strftime("%Y_%m_%d_%H_%M_%S_",time.gmtime(prev_endtime)),\
                 Prev_E_time_Frac_second, \
                 time_difference)

            if (2.05 > time_difference > 0.95): # This means the proposed file is too far forward and a second has to be removed
                St_time = time.gmtime(mkgmtime(St_time)-1)
                print "The start time for this file has been shifted back one second."
            elif (-2.05 < time_difference < -0.95): # The proposed file start time is too far backward and one second must be added
                St_time = time.gmtime(mkgmtime(St_time)+1)
                print "The start time for this file has been moved forward one second."

#
#                     Calculate the rest of the times
#

        E_time = time.gmtime(mkgmtime(St_time)+len(Data)/Sps+Frac_second)
        E_time_Frac_second = Frac_second+(len(Data)/Sps-int(len(Data)/Sps))

        if E_time_Frac_second >= 1.000: # Dont let it roll over
            E_time_Frac_second = E_time_Frac_second - 1.0

        Start_time = time.strftime("%Y_%m_%d_%H_%M_%S_",St_time)
        End_time   = time.strftime("%Y_%m_%d_%H_%M_%S_",E_time)
        Start_time +=str.format("{0:0.3f}",Frac_second)[2:]
        End_time +=str.format("{0:0.3f}",E_time_Frac_second)[2:]

        if int(len(Data)) <> int(Samplecount): # THe OUT File terminated prematurely.
        
            print " Warning! Mismatch. Number of samples found: {0} Number of samples reported: {1}".format(len(Data),Samplecount)
            print " Predicted time discontinuity, so end time is being set to zero."
            E_time_Frac_second = 0.0
            E_time = time.gmtime(0.000)

        filesec = int(infile[-6:-4])
        filemin = int(infile[-8:-6])
        filehour = int(infile[-10:-8])
        filetime = filehour*3600+filemin*60+filesec
        Timediff = filetime-St_time.tm_sec-St_time.tm_min*60-St_time.tm_hour*3600-Frac_second
        et = len(Data)/Sps # Number of samples divided by the sample rate = number of second worth of time-history

        print "Sample rate : {0:0.3f}".format(Sps)
#        print "Delta: {0:8.6e}".format(Delta)
#        print "{} seconds worth of time-history in this file.".format(et)
#        print "Number of samples found: {0} Number of samples reported: {1}".format(len(Data),Samplecount)
#        print "Official start_time: {}".format(Start_time)
        print "Calculated End_time: {}".format(End_time)
#        print "Time difference between encoded file name and calculated start time from file: {} Seconds".format(Timediff)
#        print "Time stamp = {0} {1}".format(data[0],data[1])
#        print "First impulse found at sample: {}  VALUE: {}\n\n".format(Initial_sample,GPS[Initial_sample])
#        print "Fraction of a second for start time = {0:1.3f}".format(Frac_second)
#        print "GPS5 = {}".format(GPS[5])
#        print "GPS Median 2.5 = {0}".format(Gpsmean15)
#        print "Final impulse found at sample: {} VALUE: {}".format(Final_sample,GPS[Final_sample])
#        print "Total samples between tic marks = {}".format((Final_sample-Initial_sample))
#        print "Fraction of second to add: {} second".format(Frac_second)
#        print "Total count of tickmarks: {}".format(Count)


      #  print "Sample Count from the header file: ",Samplecount

#
#           Create the header information for making miniseed files.
#
#           set current time


    
            # At this point, we have our header information in an index, and we have calculated the true sample rate, 
            # We have extracted the true start time and we've 
            # verified the true second remainder for placing into the start time.

  
#    print "Channel gains used:"
#    for i in range(4):
#        print "    Channel {0}: {1} Volts / count.".format(i,Channelgain[0])
#
#
#
#
#               Begin writing the channels out as miniseed files
#
#
        Channel = []
        for i in range(0,4):
            Channel.append(header["Ch{}ID:".format(i)])

             #                      Create each stream
        for i in range(0,4):
            stats = {'network': Network, 'station': header['A-DInfo:'], 'location': '',\
                     'channel': Channel[i], 'npts': len(Data), 'sampling_rate': Sps,\
                     'mseed': {'dataquality': 'D'}} 
            stats['starttime'] = UTCDateTime(mkgmtime(St_time)+Frac_second)
    #        print stats

    #        print "Here is the time from within stats: {}".format( stats['starttime'])

            b = np.arange(len(Data),dtype=np.float32)   #   Establishes the size of the datastream
            for n in range(len(Data)):        #   Load the array with time-history data
                b[n] = np.float32(Data[n][i]) #   Convert the measurement from counts to volts.


            f2 = outfolder+Start_time+"_"+Network+"_"+header['A-DInfo:']+"_{}.mseed".format(Channel[i])
      #      print "Name of output file is supposed to be {}".format(f2)

            if St_time.tm_year <> 1970:
            #    print "St_time.tm_year == 1970?? {}".format(St_time.tm_year)
                if Channel[i] !="UNK":   # We do not write streams in which the channel name is UNK
             #       print Channel[i]
                    st=Stream([Trace(data=b, header = stats)])
#                    print "We have set the stream as st"
                    st[0].data = st[0].data.astype('int32')  # dtype it for use with steim2
#                   print "We have set the st.data as type int32"
                    st[0].write(f2,format="MSEED", encoding = 'STEIM2')
#                    print "We are trying to write f2 as miniseed with steim2 encoding"
                    print " File successfully written: {0}".format(f2)
         

            else:
                print "There are problems with the date in this file so it is not being written to disk."
              #  print "Reported year = {}".format(St_time.tm_year)
                E_time_Frac_second = 0.0
                E_time = time.gmtime(0.000)
        print"\n\n"
        return (E_time,E_time_Frac_second) # Pass the calculated end time so that it can be used for validating the next file 

    except:
        print " Warning! File {0} has fatal errors.\n Number of samples within file =  {1}\n".format(hexfile,len(Data))
        print " Predicted time discontinuity, so end time is being set to zero."
        E_time_Frac_second = 0.0
        E_time = time.gmtime(0.000)
        return (E_time,E_time_Frac_second)

def main():

    # Go process the command line switches
    # If the switch for processing a whole directory is called, first catalog the diretory
    # then interatively run through the conversion program for as long as the list exists.
    # If the switch isn't valid, use the previous CLSs to process one specific file.
    # This code will not iterate through bottom directories. 
    # Actually, perhaps a better method would be to NOT embed the crawler into this program
    # but instead use csh and bash and pipe the outputs into thte converter

    #           MAIN PROGRAM BODY
    #  Parse the command line switches

    optioncount = len(sys.argv)
    outputfolder_defined = False
    filelist = []
    indir=""
    outfolder=""
    extension = '.out'
    S_time = time.gmtime(long(0)) # Initialize it to time of the creation of the universe
    S_time_Frac_second = 0.0
    Network = ""

    if optioncount > 1:
            # Parse out the command line arguments
            # First argument = incoming file folder
            # second argument = incoming file folder's two digit NETWORK CODE
            # third argument = Outgoing file folder (optional)
            # C:> out2mseed c:\seismo\RY_MOMR\2013\2013_02\ RY \\MSU-IRIS1\public\SeismicData\NERSP\RY_YBGR\2013\2013_02\
        if optioncount == 4:
            outputfolder_defined = True
            outfolder = sys.argv[3]
            Network = sys.argv[2]
            indir = sys.argv[1]+"\\"
            filelist = os.listdir(indir) # Collect a list of all files in the directory
            infile = filelist[0]
                 
        if optioncount == 3:    # out2sac infile/directoryname outfile/directoryname
            if ("\\" in sys.argv[2]) or ("/" in sys.argv[2]):          
                outputfolder_defined = True
                outfolder = sys.argv[2]
            else:
                Network = sys.argv[2]
                outfolder = sys.argv[1]+"\\"

            indir=sys.argv[1]+"\\"
            filelist = os.listdir(indir) # Collect a list of all files in the directory                
            infile = filelist[0]    # initialize for the first iteration

            print "Infolder and outfolder are specified as '{0}' and '{1}'".format(indir,outfolder)
            
        elif optioncount == 2:     # Out2sac inputfile/targetdirectory 
                                   # use default name for target file and directory
            indir = sys.argv[1]+"\\"
            filelist = os.listdir(indir) # Collect a list of all files in the directory
            infile = filelist[0]
            outfolder = indir


                                  # Parse through the directory and convert all out files
                                  # If indir, outdir have not yet been specified,
                                  # Specify them.
        if Network == "": 
            Network = raw_input('Please enter the two letter network designator. ')
            Network = Network[:2]
        
        for n in range(len(filelist)):
            if extension in str.lower(filelist[n]):
                print "\nConverting {}".format(filelist[n]) 
                if len(filelist)>1:
                    infile = indir+filelist[n]
                if not outputfolder_defined:
                    outfolder = infile[:infile.rfind('\\')+1]
                Prev_E_time = S_time
                Prev_E_time_Frac_second = S_time_Frac_second

                S_time,S_time_Frac_second = convert(Prev_E_time,Prev_E_time_Frac_second,infile,outfolder,Network)  # Convert the out file into sac files


    else:
        print "Useage: OUT2MSEED infolder Network outfolder"
        print "Or, OUT2MSEED infolder"
        print "No input folder has been specified."
        print "Be sure input folder contains .OUT files and a vdaq.txt station descriptor file."
        print len(sys.argv)

#Call the main loop
#
if __name__ == '__main__':
  main()

# <codecell>


